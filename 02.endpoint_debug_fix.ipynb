{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* notebook created by nov05 on 2024-12-05   \n",
    "* windows os, powershell, conda env `awsmle_py310` (no cuda)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "## **Issue**\n",
    "\n",
    "* 游릭丘멆잺 Issue solved:     \n",
    "\n",
    "  > ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateHyperParameterTuningJob \n",
    "  operation: The account-level service limit 'ml.g4dn.xlarge for training job usage' is 2 Instances, with current \n",
    "  utilization of 0 Instances and a request delta of 10 Instances. Please use AWS Service Quotas to request an \n",
    "  increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for \n",
    "  this quota.\n",
    "\n",
    "  * You can still create an HPO job with as many `max_jobs` as you want. However, the number of concurrent jobs is limited to 2 (`max_parallel_jobs=2`). For example, if your `max_jobs` is set to 20, only 2 training jobs will run at a time. If each training job takes about an hour, the entire HPO job will take at least 10 hours to complete.\n",
    "\n",
    "  * Go to `Service Quotas > AWS services > Amazon SageMaker`, search for `ml.g4dn.xlarg`.  \n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/nov05/pictures/refs/heads/master/Udacity/20241119_aws-mle-nanodegree/2024-12-03%2002_03_35-Quotas%20list%20-%20Amazon%20SageMaker%20_%20AWS%20Service%20Quotas.jpg\" width=600>  \n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/nov05/pictures/refs/heads/master/Udacity/20241119_aws-mle-nanodegree/2024-12-03%2002_06_13-Quotas%20list%20-%20Amazon%20SageMaker%20_%20AWS%20Service%20Quotas.jpg\" width=600>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "## **Issue**  \n",
    "\n",
    "* 游릭丘멆잺 Issue solved: This cell keeps running and doesn't return. The endpoint CloudWatch log shows 500 return code. If remove the `scripts\\inference.py` file, it shows 200.   \n",
    "\n",
    "    ```text\n",
    "    2024-12-05T14:18:14,087 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._model = self._run_handler_function(self._model_fn, *(model_dir,))  \n",
    "    2024-12-05T14:18:14,087 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     raise ModelLoadError(  \n",
    "    2024-12-05T14:18:14,087 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.9/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 80, in default_model_fn  \n",
    "    2024-12-05T14:20:18,535 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:53522 \"GET /ping HTTP/1.1\" 500 1\n",
    "    ```   \n",
    "\n",
    "    ```text \n",
    "    2024-12-05T14:18:14,087 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - sagemaker_pytorch_serving_container.default_pytorch_inference_handler.ModelLoadError: Failed to load /tmp/models/d40fd8f5cf5f48fb9dfa71137e4db3d9/model/model.pth. Please ensure model is saved using torchscript.   \n",
    "    ```\n",
    "\n",
    "* Solution: Using the code you've provided will cause an issue when loading the model in SageMaker for inference, because torch.save(model.state_dict(), f) only saves the model's state dictionary (i.e., its parameters), not the complete model architecture. SageMaker expects the model to be saved in TorchScript format (or as a complete PyTorch model including both architecture and weights) for inference.  \n",
    "\n",
    "    ```python\n",
    "    ## TODO: Save the trained model\n",
    "    path = os.path.join(args.model_dir, 'model.pth')\n",
    "    with open(path, 'wb') as f:\n",
    "        torch.save(model.state_dict(), f)\n",
    "    print(f\"Model saved at '{path}'\")\n",
    "    ```\n",
    "\n",
    "* Reference:  \n",
    "    * https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html  \n",
    "      > TorchScript is an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the `scripts\\train.py` file, change the code to the following one.  \n",
    "\n",
    "    ```python\n",
    "    def save(model, model_dir, model_name='model.pt'):\n",
    "        ## 丘멆잺 Please ensure model is saved using torchscript.\n",
    "        model.eval()\n",
    "        path = os.path.join(model_dir, model_name)\n",
    "        ## save model weights\n",
    "        # with open(path, 'wb') as f:\n",
    "        #     torch.save(model.state_dict(), f)\n",
    "        ## If your model is simple and has a straightforward forward pass, use torch.jit.trace\n",
    "        # example_input = torch.randn(1, 3, 224, 224)\n",
    "        # traced_model = torch.jit.trace(model, example_input)\n",
    "        # traced_model.save(path)\n",
    "        ## If your model has dynamic control flow (like if statements based on input), use torch.jit.script\n",
    "        scripted_model = torch.jit.script(model)\n",
    "        scripted_model.save(path) \n",
    "        print(f\"Model saved at '{path}'\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As for the model weights saved in `args.model_dir`, download and load it with the original model structure, convert the model to TorchScript.   \n",
    "    * AWS S3 URI: `s3://p3-dog-breed-image-classification/jobs/p3-dog-breeds-debug-20241204-124107/output/model.tar.gz`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\github\\\\udacity-CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## current dir\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x model.pth\n"
     ]
    }
   ],
   "source": [
    "## unpack the file\n",
    "!tar -xzvf data\\models\\resnet50_best.tar.gz -C data\\models\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename the model file\n",
    "import os\n",
    "os.rename(r'data\\\\models\\\\model.pth', r'data\\\\models\\\\old_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.53 s\n",
      "Wall time: 4.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "old_model_file = r\"data\\\\models\\\\old_model.pth\"\n",
    "model_type = 'resnet50'\n",
    "num_classes = 133\n",
    "model = getattr(torchvision.models, model_type)(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.load_state_dict(torch.load(old_model_file, map_location=torch.device('cpu')))\n",
    "model.eval()  # Put model in evaluation mode\n",
    "scripted_model = torch.jit.script(model)\n",
    "model_file = r\"data\\\\models\\\\model.pth\"\n",
    "scripted_model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czvf data/models/model.tar.gz data/models/model.pth\n",
    "## in windows wsl\n",
    "# gzip -c model.pth > model.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "\n",
    "## **Issue**\n",
    "\n",
    "* **游릭丘멆잺 Issue:**  \n",
    "    ```text\n",
    "    2024-12-05T18:36:37,292 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.9/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 73, in default_model_fn\n",
    "    2024-12-05T18:36:37,292 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     raise ValueError(\n",
    "    2024-12-05T17:34:29,292 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ValueError: Exactly one .pth or .pt file is required for PyTorch models: []  \n",
    "    ```\n",
    "\n",
    "* **Solution**: Use custome function `model_fn` in `inference.py`.   \n",
    "\n",
    "    https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py   \n",
    "\n",
    "    > For PyTorch, a default function to load a model only if Elastic Inference is used.  \n",
    "    > In other cases, users should provide customized model_fn() in script.  \n",
    "\n",
    "    ```python  \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_path = os.path.join(model_dir, DEFAULT_MODEL_FILENAME)\n",
    "    if not os.path.exists(model_path):\n",
    "        model_files = [file for file in os.listdir(model_dir) if self._is_model_file(file)]\n",
    "        if len(model_files) != 1:\n",
    "            raise ValueError(\n",
    "                \"Exactly one .pth or .pt file is required for PyTorch models: {}\".format(model_files)\n",
    "            )\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read the endpoint container logs in AWS CloudWatch  \n",
    " \n",
    "  <img src=\"https://raw.githubusercontent.com/nov05/pictures/refs/heads/master/Udacity/20241119_aws-mle-nanodegree/2024-12-05%2013_54_00-CloudWatch%20_%20us-east-1.jpg\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input arg model_dir=`/tmp/models/4765a8173953463fa048dfd3f5c0f889/model`\n",
    "    ```text\n",
    "    2024-12-05T20:07:11,559 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - 游녤 Model path: /tmp/models/4765a8173953463fa048dfd3f5c0f889/model/model.pth` \n",
    "    ```\n",
    "\n",
    "* In this case, there is no `model.pth`, which might caused by the improper packaging the file into `.tar.gz`.   \n",
    "\n",
    "    ```text\n",
    "    2024-12-05T20:29:09,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - 游녤 Model dir: /tmp/models/5a7d4e053bc64df2a6a385971f122d3c/model, type: <class 'str'>   \n",
    "    2024-12-05T20:29:09,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Directory: data    \n",
    "    2024-12-05T20:29:09,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Directory: code   \n",
    "    ```\n",
    "\n",
    "* It should look like this.   \n",
    "\n",
    "    ```text\n",
    "    2024-12-05T22:38:18,849 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
    "    2024-12-05T22:38:18,979 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - 游릭 Loading model...\n",
    "    2024-12-05T22:38:18,980 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - 游녤 Device: cpu\n",
    "    2024-12-05T22:38:18,981 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - 游녤 Model dir: /opt/ml/model, type: <class 'str'>\n",
    "    2024-12-05T22:38:18,982 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - File: model.pth\n",
    "    2024-12-05T22:38:18,982 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Directory: code\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "## **Issue** \n",
    "\n",
    "\n",
    "> ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary and could not load the entire response body. See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/p3-dog-breed-classification in account 852125600954 for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sagemaker_inference\n",
    "## Successfully installed retrying-1.3.4 sagemaker_inference-1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['application/json', 'text/csv']\n"
     ]
    }
   ],
   "source": [
    "from sagemaker_inference import content_types, decoder\n",
    "print(content_types.UTF8_TYPES)\n",
    "# np_array = decoder.decode(input_data, content_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[219, 233, 234],\n",
       "       [224, 238, 239],\n",
       "       [223, 237, 238]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "str = \"[[219, 233, 234], [224, 238, 239], [223, 237, 238]]\"\n",
    "data = np.array(json.loads(str))\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'debug': False, 'wandb': True}\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.debug = False\n",
    "config = Config()\n",
    "new_config_dict = {\"wandb\": True}\n",
    "for key, value in new_config_dict.items():\n",
    "    setattr(config, key, value)\n",
    "print(config.__dict__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awsmle_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
