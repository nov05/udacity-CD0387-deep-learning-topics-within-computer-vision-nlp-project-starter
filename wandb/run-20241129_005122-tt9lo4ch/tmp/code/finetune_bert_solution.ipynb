{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079808d1",
   "metadata": {},
   "source": [
    "* changed by nov05 on 2024-11-28  \n",
    "* Udacity AWS MLE Nanodegree (ND189)  \n",
    "  Course 4, 3.15 Excercise: Fine-Tuning BERT    \n",
    "* local env `conda activate cuda_py310` with cuda enabled  \n",
    "* [CoLA dataset on KaggleHub](https://www.kaggle.com/datasets/krazy47/cola-the-corpus-of-linguistic-acceptability)  \n",
    "  The Corpus of Linguistic Acceptability   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5575d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\github\\udacity-CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d:\\\\github\\\\udacity-CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use the repo root folder as working directory\n",
    "## training data is in data\\\n",
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7b442",
   "metadata": {},
   "source": [
    "## Solution: Fine-tune BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20247d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import sys\n",
    "# import json\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "# import torch.distributed as dist\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer # type: ignore\n",
    "from transformers import get_linear_schedule_with_warmup # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "\n",
    "## log training process with W&B if uncommented\n",
    "# os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23828261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ Running on device type: cuda:0\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # self.wandb = True\n",
    "        self.device = torch.device('cpu')\n",
    "        self.max_len = 64 ## this is the max length of the sentence\n",
    "        self.epochs = 30\n",
    "        self.batch_size = 32\n",
    "        self.opt_lr = 2e-5\n",
    "        self.opt_weight_decay = 1e-3\n",
    "\n",
    "config = Config()\n",
    "config.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ‘‰ Running on device type: {config.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a15cc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8551,) (8551,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6550</th>\n",
       "      <td>0</td>\n",
       "      <td>Talkative and a bully entered.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8541</th>\n",
       "      <td>1</td>\n",
       "      <td>Where has he put the cake?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7311</th>\n",
       "      <td>1</td>\n",
       "      <td>I know that she runs.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                        sentence\n",
       "6550      0  Talkative and a bully entered.\n",
       "8541      1      Where has he put the cake?\n",
       "7311      1           I know that she runs."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "   r\"cd0387_common_model_arch_types_fine_tuning\\cola_public\\raw\\in_domain_train.tsv\",\n",
    "   sep=\"\\t\",\n",
    "   header=None,\n",
    "   usecols=[1, 3],\n",
    "   names=[\"label\", \"sentence\"],\n",
    ")\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "print(sentences.shape, labels.shape)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c7d8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df)\n",
    "train_df.to_csv(r\"data\\cola\\train.csv\", index=False)\n",
    "test_df.to_csv(r\"data\\cola\\test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22c3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnov05\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e557c75f4741c58ab446d485a43253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777777626, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\github\\udacity-CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter\\wandb\\run-20241129_005122-tt9lo4ch</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nov05/udacity-awsmle-bert-cola/runs/tt9lo4ch' target=\"_blank\">swept-leaf-2</a></strong> to <a href='https://wandb.ai/nov05/udacity-awsmle-bert-cola' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nov05/udacity-awsmle-bert-cola' target=\"_blank\">https://wandb.ai/nov05/udacity-awsmle-bert-cola</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nov05/udacity-awsmle-bert-cola/runs/tt9lo4ch' target=\"_blank\">https://wandb.ai/nov05/udacity-awsmle-bert-cola/runs/tt9lo4ch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ Train Epoch 0:\n",
      "Step 1: [0/6413 (0%)] Loss: 0.656576\n",
      "Step 11: [320/6413 (5%)] Loss: 0.618167\n",
      "Step 21: [640/6413 (10%)] Loss: 0.601939\n",
      "Step 31: [960/6413 (15%)] Loss: 0.538861\n",
      "Step 41: [1280/6413 (20%)] Loss: 0.603383\n",
      "Step 51: [1600/6413 (25%)] Loss: 0.620070\n",
      "Step 61: [1920/6413 (30%)] Loss: 0.589405\n",
      "Step 71: [2240/6413 (35%)] Loss: 0.671709\n",
      "Step 81: [2560/6413 (40%)] Loss: 0.690177\n",
      "Step 91: [2880/6413 (45%)] Loss: 0.579779\n",
      "Step 101: [3200/6413 (50%)] Loss: 0.613103\n",
      "Step 111: [3520/6413 (55%)] Loss: 0.621339\n",
      "Step 121: [3840/6413 (60%)] Loss: 0.558984\n",
      "Step 131: [4160/6413 (65%)] Loss: 0.741495\n",
      "Step 141: [4480/6413 (70%)] Loss: 0.499705\n",
      "Step 151: [4800/6413 (75%)] Loss: 0.620361\n",
      "Step 161: [5120/6413 (80%)] Loss: 0.574555\n",
      "Step 171: [5440/6413 (85%)] Loss: 0.583832\n",
      "Step 181: [5760/6413 (90%)] Loss: 0.718180\n",
      "Step 191: [6080/6413 (95%)] Loss: 0.590198\n",
      "Step 201: [2600/6413 (100%)] Loss: 0.488183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 1/30 [01:16<36:59, 76.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Test Accuracy:  0.022436272217025256\n",
      "ðŸ‘‰ Train Epoch 1:\n",
      "Step 202: [0/6413 (0%)] Loss: 0.564005\n",
      "Step 212: [320/6413 (5%)] Loss: 0.620965\n",
      "Step 222: [640/6413 (10%)] Loss: 0.502507\n",
      "Step 232: [960/6413 (15%)] Loss: 0.662596\n",
      "Step 242: [1280/6413 (20%)] Loss: 0.510011\n",
      "Step 252: [1600/6413 (25%)] Loss: 0.687839\n",
      "Step 262: [1920/6413 (30%)] Loss: 0.443649\n",
      "Step 272: [2240/6413 (35%)] Loss: 0.480629\n",
      "Step 282: [2560/6413 (40%)] Loss: 0.672007\n",
      "Step 292: [2880/6413 (45%)] Loss: 0.476020\n",
      "Step 302: [3200/6413 (50%)] Loss: 0.660508\n",
      "Step 312: [3520/6413 (55%)] Loss: 0.456599\n",
      "Step 322: [3840/6413 (60%)] Loss: 0.418791\n",
      "Step 332: [4160/6413 (65%)] Loss: 0.548193\n",
      "Step 342: [4480/6413 (70%)] Loss: 0.570143\n",
      "Step 352: [4800/6413 (75%)] Loss: 0.382537\n",
      "Step 362: [5120/6413 (80%)] Loss: 0.585470\n",
      "Step 372: [5440/6413 (85%)] Loss: 0.682845\n",
      "Step 382: [5760/6413 (90%)] Loss: 0.623224\n",
      "Step 392: [6080/6413 (95%)] Loss: 0.459819\n",
      "Step 402: [2600/6413 (100%)] Loss: 0.560218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 2/30 [03:43<54:55, 117.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Test Accuracy:  0.023931648917032453\n",
      "ðŸ‘‰ Train Epoch 2:\n",
      "Step 403: [0/6413 (0%)] Loss: 0.655588\n",
      "Step 413: [320/6413 (5%)] Loss: 0.684162\n",
      "Step 423: [640/6413 (10%)] Loss: 0.499493\n",
      "Step 433: [960/6413 (15%)] Loss: 0.543537\n",
      "Step 443: [1280/6413 (20%)] Loss: 0.520505\n",
      "Step 453: [1600/6413 (25%)] Loss: 0.517777\n",
      "Step 463: [1920/6413 (30%)] Loss: 0.394036\n",
      "Step 473: [2240/6413 (35%)] Loss: 0.530707\n",
      "Step 483: [2560/6413 (40%)] Loss: 0.481557\n",
      "Step 493: [2880/6413 (45%)] Loss: 0.520182\n",
      "Step 503: [3200/6413 (50%)] Loss: 0.456944\n",
      "Step 513: [3520/6413 (55%)] Loss: 0.623784\n",
      "Step 523: [3840/6413 (60%)] Loss: 0.488163\n",
      "Step 533: [4160/6413 (65%)] Loss: 0.656875\n",
      "Step 543: [4480/6413 (70%)] Loss: 0.696289\n",
      "Step 553: [4800/6413 (75%)] Loss: 0.617890\n",
      "Step 563: [5120/6413 (80%)] Loss: 0.461889\n",
      "Step 573: [5440/6413 (85%)] Loss: 0.511563\n",
      "Step 583: [5760/6413 (90%)] Loss: 0.506416\n",
      "Step 593: [6080/6413 (95%)] Loss: 0.515954\n",
      "Step 603: [2600/6413 (100%)] Loss: 0.287057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 3/30 [05:09<46:30, 103.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Test Accuracy:  0.024028342448010363\n",
      "ðŸ‘‰ Train Epoch 3:\n",
      "Step 604: [0/6413 (0%)] Loss: 0.377173\n",
      "Step 614: [320/6413 (5%)] Loss: 0.563180\n",
      "Step 624: [640/6413 (10%)] Loss: 0.508864\n",
      "Step 634: [960/6413 (15%)] Loss: 0.467197\n",
      "Step 644: [1280/6413 (20%)] Loss: 0.477159\n",
      "Step 654: [1600/6413 (25%)] Loss: 0.719676\n",
      "Step 664: [1920/6413 (30%)] Loss: 0.461570\n",
      "Step 674: [2240/6413 (35%)] Loss: 0.430466\n",
      "Step 684: [2560/6413 (40%)] Loss: 0.509728\n",
      "Step 694: [2880/6413 (45%)] Loss: 0.535663\n",
      "Step 704: [3200/6413 (50%)] Loss: 0.333321\n",
      "Step 714: [3520/6413 (55%)] Loss: 0.330852\n",
      "Step 724: [3840/6413 (60%)] Loss: 0.614688\n",
      "Step 734: [4160/6413 (65%)] Loss: 0.556368\n",
      "Step 744: [4480/6413 (70%)] Loss: 0.334253\n",
      "Step 754: [4800/6413 (75%)] Loss: 0.370912\n",
      "Step 764: [5120/6413 (80%)] Loss: 0.552159\n",
      "Step 774: [5440/6413 (85%)] Loss: 0.328531\n",
      "Step 784: [5760/6413 (90%)] Loss: 0.620098\n",
      "Step 794: [6080/6413 (95%)] Loss: 0.577326\n",
      "Step 804: [2600/6413 (100%)] Loss: 0.393525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 4/30 [08:51<1:05:02, 150.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Test Accuracy:  0.024246465064402388\n",
      "ðŸ‘‰ Train Epoch 4:\n",
      "Step 805: [0/6413 (0%)] Loss: 0.437169\n",
      "Step 815: [320/6413 (5%)] Loss: 0.366969\n",
      "Step 825: [640/6413 (10%)] Loss: 0.545591\n",
      "Step 835: [960/6413 (15%)] Loss: 0.527032\n",
      "Step 845: [1280/6413 (20%)] Loss: 0.582812\n",
      "Step 855: [1600/6413 (25%)] Loss: 0.514167\n",
      "Step 865: [1920/6413 (30%)] Loss: 0.457168\n",
      "Step 875: [2240/6413 (35%)] Loss: 0.326938\n",
      "Step 885: [2560/6413 (40%)] Loss: 0.408326\n",
      "Step 895: [2880/6413 (45%)] Loss: 0.507172\n",
      "Step 905: [3200/6413 (50%)] Loss: 0.588444\n",
      "Step 915: [3520/6413 (55%)] Loss: 0.414341\n",
      "Step 925: [3840/6413 (60%)] Loss: 0.502352\n",
      "Step 935: [4160/6413 (65%)] Loss: 0.518117\n",
      "Step 945: [4480/6413 (70%)] Loss: 0.352987\n",
      "Step 955: [4800/6413 (75%)] Loss: 0.283002\n",
      "Step 965: [5120/6413 (80%)] Loss: 0.562468\n",
      "Step 975: [5440/6413 (85%)] Loss: 0.350719\n",
      "Step 985: [5760/6413 (90%)] Loss: 0.598661\n",
      "Step 995: [6080/6413 (95%)] Loss: 0.564264\n",
      "Step 1005: [2600/6413 (100%)] Loss: 0.476096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 5/30 [10:12<52:15, 125.42s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Test Accuracy:  0.024644482622148665\n",
      "ðŸ‘‰ Train Epoch 5:\n",
      "Step 1006: [0/6413 (0%)] Loss: 0.615279\n",
      "Step 1016: [320/6413 (5%)] Loss: 0.662243\n",
      "Step 1026: [640/6413 (10%)] Loss: 0.470574\n",
      "Step 1036: [960/6413 (15%)] Loss: 0.422844\n",
      "Step 1046: [1280/6413 (20%)] Loss: 0.501589\n",
      "Step 1056: [1600/6413 (25%)] Loss: 0.553367\n",
      "Step 1066: [1920/6413 (30%)] Loss: 0.511237\n",
      "Step 1076: [2240/6413 (35%)] Loss: 0.466602\n",
      "Step 1086: [2560/6413 (40%)] Loss: 0.565345\n",
      "Step 1096: [2880/6413 (45%)] Loss: 0.428859\n",
      "Step 1106: [3200/6413 (50%)] Loss: 0.335829\n",
      "Step 1116: [3520/6413 (55%)] Loss: 0.478955\n",
      "Step 1126: [3840/6413 (60%)] Loss: 0.351724\n",
      "Step 1136: [4160/6413 (65%)] Loss: 0.532083\n",
      "Step 1146: [4480/6413 (70%)] Loss: 0.299963\n",
      "Step 1156: [4800/6413 (75%)] Loss: 0.506489\n",
      "Step 1166: [5120/6413 (80%)] Loss: 0.403715\n",
      "Step 1176: [5440/6413 (85%)] Loss: 0.550961\n",
      "Step 1186: [5760/6413 (90%)] Loss: 0.406121\n",
      "Step 1196: [6080/6413 (95%)] Loss: 0.336315\n",
      "Step 1206: [2600/6413 (100%)] Loss: 0.687900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 6/30 [12:59<55:48, 139.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Test Accuracy:  0.02448819889184716\n",
      "ðŸ‘‰ Train Epoch 6:\n",
      "Step 1207: [0/6413 (0%)] Loss: 0.449853\n",
      "Step 1217: [320/6413 (5%)] Loss: 0.460226\n",
      "Step 1227: [640/6413 (10%)] Loss: 0.378208\n",
      "Step 1237: [960/6413 (15%)] Loss: 0.311565\n",
      "Step 1247: [1280/6413 (20%)] Loss: 0.362131\n",
      "Step 1257: [1600/6413 (25%)] Loss: 0.438379\n",
      "Step 1267: [1920/6413 (30%)] Loss: 0.372230\n",
      "Step 1277: [2240/6413 (35%)] Loss: 0.458886\n",
      "Step 1287: [2560/6413 (40%)] Loss: 0.378277\n",
      "Step 1297: [2880/6413 (45%)] Loss: 0.537801\n",
      "Step 1307: [3200/6413 (50%)] Loss: 0.513159\n",
      "Step 1317: [3520/6413 (55%)] Loss: 0.377542\n",
      "Step 1327: [3840/6413 (60%)] Loss: 0.439138\n",
      "Step 1337: [4160/6413 (65%)] Loss: 0.749815\n",
      "Step 1347: [4480/6413 (70%)] Loss: 0.583158\n",
      "Step 1357: [4800/6413 (75%)] Loss: 0.351560\n",
      "Step 1367: [5120/6413 (80%)] Loss: 0.465992\n",
      "Step 1377: [5440/6413 (85%)] Loss: 0.481880\n",
      "Step 1387: [5760/6413 (90%)] Loss: 0.490741\n",
      "Step 1397: [6080/6413 (95%)] Loss: 0.579278\n",
      "Step 1407: [2600/6413 (100%)] Loss: 0.499177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 7/30 [15:36<55:42, 145.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Test Accuracy:  0.02484236705763834\n",
      "ðŸ‘‰ Train Epoch 7:\n",
      "Step 1408: [0/6413 (0%)] Loss: 0.423743\n",
      "Step 1418: [320/6413 (5%)] Loss: 0.526868\n",
      "Step 1428: [640/6413 (10%)] Loss: 0.559795\n",
      "Step 1438: [960/6413 (15%)] Loss: 0.263112\n",
      "Step 1448: [1280/6413 (20%)] Loss: 0.348564\n",
      "Step 1458: [1600/6413 (25%)] Loss: 0.566075\n",
      "Step 1468: [1920/6413 (30%)] Loss: 0.381011\n",
      "Step 1478: [2240/6413 (35%)] Loss: 0.410372\n",
      "Step 1488: [2560/6413 (40%)] Loss: 0.396186\n",
      "Step 1498: [2880/6413 (45%)] Loss: 0.577733\n",
      "Step 1508: [3200/6413 (50%)] Loss: 0.430088\n",
      "Step 1518: [3520/6413 (55%)] Loss: 0.425598\n",
      "Step 1528: [3840/6413 (60%)] Loss: 0.455894\n",
      "Step 1538: [4160/6413 (65%)] Loss: 0.461627\n",
      "Step 1548: [4480/6413 (70%)] Loss: 0.561792\n",
      "Step 1558: [4800/6413 (75%)] Loss: 0.344328\n",
      "Step 1568: [5120/6413 (80%)] Loss: 0.531204\n",
      "Step 1578: [5440/6413 (85%)] Loss: 0.423423\n",
      "Step 1588: [5760/6413 (90%)] Loss: 0.263932\n",
      "Step 1598: [6080/6413 (95%)] Loss: 0.428575\n",
      "Step 1608: [2600/6413 (100%)] Loss: 0.465635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 8/30 [17:02<46:19, 126.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Test Accuracy:  0.02467821292365259\n",
      "ðŸ‘‰ Train Epoch 8:\n",
      "Step 1609: [0/6413 (0%)] Loss: 0.500540\n",
      "Step 1619: [320/6413 (5%)] Loss: 0.431586\n",
      "Step 1629: [640/6413 (10%)] Loss: 0.484081\n",
      "Step 1639: [960/6413 (15%)] Loss: 0.273646\n",
      "Step 1649: [1280/6413 (20%)] Loss: 0.448507\n",
      "Step 1659: [1600/6413 (25%)] Loss: 0.237001\n",
      "Step 1669: [1920/6413 (30%)] Loss: 0.272892\n",
      "Step 1679: [2240/6413 (35%)] Loss: 0.471091\n",
      "Step 1689: [2560/6413 (40%)] Loss: 0.327275\n",
      "Step 1699: [2880/6413 (45%)] Loss: 0.274417\n",
      "Step 1709: [3200/6413 (50%)] Loss: 0.389423\n",
      "Step 1719: [3520/6413 (55%)] Loss: 0.331657\n",
      "Step 1729: [3840/6413 (60%)] Loss: 0.475019\n",
      "Step 1739: [4160/6413 (65%)] Loss: 0.411533\n",
      "Step 1749: [4480/6413 (70%)] Loss: 0.319233\n",
      "Step 1759: [4800/6413 (75%)] Loss: 0.490705\n",
      "Step 1769: [5120/6413 (80%)] Loss: 0.459494\n",
      "Step 1779: [5440/6413 (85%)] Loss: 0.525613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 8/30 [19:11<52:47, 143.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 169\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m eval_accuracy\n\u001b[0;32m    164\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# set the wandb project where this run will be logged\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mudacity-awsmle-bert-cola\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    167\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m    168\u001b[0m )\n\u001b[1;32m--> 169\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 126\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(b_input_ids, token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m    123\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mb_input_mask, labels\u001b[38;5;241m=\u001b[39mb_labels)\n\u001b[0;32m    124\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 126\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem()}, step\u001b[38;5;241m=\u001b[39mtotal_steps)\n\u001b[0;32m    128\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def get_train_data_loader(batch_size):\n",
    "    dataset = pd.read_csv(os.path.join(\"data\", \"cola\", \"train.csv\"))\n",
    "    sentences = dataset.sentence.values\n",
    "    labels = dataset.label.values\n",
    "\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for id in input_ids:\n",
    "        while len(id) < config.max_len:\n",
    "            id.append(0)\n",
    "        input_ids_padded.append(id)\n",
    "    input_ids = input_ids_padded\n",
    "\n",
    "    # mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    # convert to PyTorch data types.\n",
    "    train_inputs = torch.tensor(input_ids)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def get_test_data_loader(test_batch_size):\n",
    "    dataset = pd.read_csv(os.path.join(\"data\", \"cola\", \"test.csv\"))\n",
    "    sentences = dataset.sentence.values\n",
    "    labels = dataset.label.values\n",
    "\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for id in input_ids:\n",
    "        while len(id) < config.max_len:\n",
    "            id.append(0)\n",
    "        input_ids_padded.append(id)\n",
    "    input_ids = input_ids_padded\n",
    "\n",
    "    # mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    # convert to PyTorch data types.\n",
    "    train_inputs = torch.tensor(input_ids)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=test_batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def train():\n",
    "    train_loader = get_train_data_loader(config.batch_size)\n",
    "    test_loader = get_test_data_loader(config.batch_size)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels=2,  # The number of output labels--2 for binary classification.\n",
    "        output_attentions=False,  # Whether the model returns attentions weights\n",
    "        output_hidden_states=False,  # Whether the model returns all hidden-states\n",
    "    )\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False  # Freeze all BERT layers\n",
    "    for param in model.bert.encoder.layer[-1].parameters():\n",
    "        param.requires_grad = True  # Unfreeze the last layer\n",
    "\n",
    "    model = model.to(config.device)\n",
    "    ## set up optimizer\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.bert.encoder.layer[-1].named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.01,  # Apply weight decay to weights\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.bert.encoder.layer[-1].named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,  # No weight decay for biases and LayerNorms\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.classifier.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.01,  # Apply weight decay to classifier weights\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.classifier.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,  # No weight decay for classifier biases and LayerNorms\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(1e3),  # You can start with 0 warmup steps, or adjust this\n",
    "        num_training_steps=len(train_loader)*config.epochs\n",
    "    )\n",
    "    total_steps = 0\n",
    "\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        print(f\"ðŸ‘‰ Train Epoch {epoch}:\")\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            total_steps += 1\n",
    "            b_input_ids = batch[0].to(config.device)\n",
    "            b_input_mask = batch[1].to(config.device)\n",
    "            b_labels = batch[2].to(config.device)\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, \n",
    "                attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            wandb.log({\"train_loss\": loss.item()}, step=total_steps)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "            if step%10==0:\n",
    "                print(\n",
    "                    f\"Step {total_steps}: \" \n",
    "                    f\"[{step*len(batch[0])}/{len(train_loader.sampler)} \" \n",
    "                    f\"({(100.0*step/len(train_loader)):.0f}%)] \" \n",
    "                    f\"Loss: {loss.item():.6f}\"\n",
    "                )\n",
    "        scheduler.step()\n",
    "        eval_accuracy = test(model, test_loader)\n",
    "        wandb.log({f\"eval_accuracy_epoch (%)\": eval_accuracy*100}, step=total_steps)\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    _, eval_accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            b_input_ids = batch[0].to(config.device)\n",
    "            b_input_mask = batch[1].to(config.device)\n",
    "            b_labels = batch[2].to(config.device)\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to(\"cpu\").numpy()\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "    eval_accuracy /= len(test_loader.dataset)\n",
    "    print(\"ðŸŸ¢ Test Accuracy: \", eval_accuracy)\n",
    "    return eval_accuracy\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"udacity-awsmle-bert-cola\",\n",
    "    config=config\n",
    ")\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf3539",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
