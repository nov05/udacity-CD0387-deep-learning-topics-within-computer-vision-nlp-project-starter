Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  3%|â–Ž         | 1/30 [01:16<36:59, 76.55s/it]
ðŸ‘‰ Train Epoch 0:
Step 1: [0/6413 (0%)] Loss: 0.656576
Step 11: [320/6413 (5%)] Loss: 0.618167
Step 21: [640/6413 (10%)] Loss: 0.601939
Step 31: [960/6413 (15%)] Loss: 0.538861
Step 41: [1280/6413 (20%)] Loss: 0.603383
Step 51: [1600/6413 (25%)] Loss: 0.620070
Step 61: [1920/6413 (30%)] Loss: 0.589405
Step 71: [2240/6413 (35%)] Loss: 0.671709
Step 81: [2560/6413 (40%)] Loss: 0.690177
Step 91: [2880/6413 (45%)] Loss: 0.579779
Step 101: [3200/6413 (50%)] Loss: 0.613103
Step 111: [3520/6413 (55%)] Loss: 0.621339
Step 121: [3840/6413 (60%)] Loss: 0.558984
Step 131: [4160/6413 (65%)] Loss: 0.741495
Step 141: [4480/6413 (70%)] Loss: 0.499705
Step 151: [4800/6413 (75%)] Loss: 0.620361
Step 161: [5120/6413 (80%)] Loss: 0.574555
Step 171: [5440/6413 (85%)] Loss: 0.583832
Step 181: [5760/6413 (90%)] Loss: 0.718180
Step 191: [6080/6413 (95%)] Loss: 0.590198
Step 201: [2600/6413 (100%)] Loss: 0.488183
ðŸŸ¢ Test Accuracy:  0.022436272217025256
ðŸ‘‰ Train Epoch 1:
Step 202: [0/6413 (0%)] Loss: 0.564005
Step 212: [320/6413 (5%)] Loss: 0.620965
Step 222: [640/6413 (10%)] Loss: 0.502507
Step 232: [960/6413 (15%)] Loss: 0.662596
Step 242: [1280/6413 (20%)] Loss: 0.510011
Step 252: [1600/6413 (25%)] Loss: 0.687839
Step 262: [1920/6413 (30%)] Loss: 0.443649
Step 272: [2240/6413 (35%)] Loss: 0.480629
Step 282: [2560/6413 (40%)] Loss: 0.672007
Step 292: [2880/6413 (45%)] Loss: 0.476020
Step 302: [3200/6413 (50%)] Loss: 0.660508
Step 312: [3520/6413 (55%)] Loss: 0.456599
Step 322: [3840/6413 (60%)] Loss: 0.418791
Step 332: [4160/6413 (65%)] Loss: 0.548193
Step 342: [4480/6413 (70%)] Loss: 0.570143
Step 352: [4800/6413 (75%)] Loss: 0.382537
Step 362: [5120/6413 (80%)] Loss: 0.585470
Step 372: [5440/6413 (85%)] Loss: 0.682845
Step 382: [5760/6413 (90%)] Loss: 0.623224
Step 392: [6080/6413 (95%)] Loss: 0.459819
Step 402: [2600/6413 (100%)] Loss: 0.560218
ðŸŸ¢ Test Accuracy:  0.023931648917032453
ðŸ‘‰ Train Epoch 2:
Step 403: [0/6413 (0%)] Loss: 0.655588
Step 413: [320/6413 (5%)] Loss: 0.684162
Step 423: [640/6413 (10%)] Loss: 0.499493
Step 433: [960/6413 (15%)] Loss: 0.543537
Step 443: [1280/6413 (20%)] Loss: 0.520505
Step 453: [1600/6413 (25%)] Loss: 0.517777
Step 463: [1920/6413 (30%)] Loss: 0.394036
Step 473: [2240/6413 (35%)] Loss: 0.530707
Step 483: [2560/6413 (40%)] Loss: 0.481557
Step 493: [2880/6413 (45%)] Loss: 0.520182
Step 503: [3200/6413 (50%)] Loss: 0.456944
Step 513: [3520/6413 (55%)] Loss: 0.623784
Step 523: [3840/6413 (60%)] Loss: 0.488163
Step 533: [4160/6413 (65%)] Loss: 0.656875
Step 543: [4480/6413 (70%)] Loss: 0.696289
Step 553: [4800/6413 (75%)] Loss: 0.617890
Step 563: [5120/6413 (80%)] Loss: 0.461889
Step 573: [5440/6413 (85%)] Loss: 0.511563
Step 583: [5760/6413 (90%)] Loss: 0.506416
Step 593: [6080/6413 (95%)] Loss: 0.515954
Step 603: [2600/6413 (100%)] Loss: 0.287057
ðŸŸ¢ Test Accuracy:  0.024028342448010363
ðŸ‘‰ Train Epoch 3:
Step 604: [0/6413 (0%)] Loss: 0.377173
Step 614: [320/6413 (5%)] Loss: 0.563180
Step 624: [640/6413 (10%)] Loss: 0.508864
Step 634: [960/6413 (15%)] Loss: 0.467197
Step 644: [1280/6413 (20%)] Loss: 0.477159
Step 654: [1600/6413 (25%)] Loss: 0.719676
Step 664: [1920/6413 (30%)] Loss: 0.461570
Step 674: [2240/6413 (35%)] Loss: 0.430466
Step 684: [2560/6413 (40%)] Loss: 0.509728
Step 694: [2880/6413 (45%)] Loss: 0.535663
Step 704: [3200/6413 (50%)] Loss: 0.333321
Step 714: [3520/6413 (55%)] Loss: 0.330852
Step 724: [3840/6413 (60%)] Loss: 0.614688
Step 734: [4160/6413 (65%)] Loss: 0.556368
Step 744: [4480/6413 (70%)] Loss: 0.334253
Step 754: [4800/6413 (75%)] Loss: 0.370912
Step 764: [5120/6413 (80%)] Loss: 0.552159
Step 774: [5440/6413 (85%)] Loss: 0.328531
Step 784: [5760/6413 (90%)] Loss: 0.620098
Step 794: [6080/6413 (95%)] Loss: 0.577326
Step 804: [2600/6413 (100%)] Loss: 0.393525
ðŸŸ¢ Test Accuracy:  0.024246465064402388
ðŸ‘‰ Train Epoch 4:
Step 805: [0/6413 (0%)] Loss: 0.437169
Step 815: [320/6413 (5%)] Loss: 0.366969
Step 825: [640/6413 (10%)] Loss: 0.545591
Step 835: [960/6413 (15%)] Loss: 0.527032
Step 845: [1280/6413 (20%)] Loss: 0.582812
Step 855: [1600/6413 (25%)] Loss: 0.514167
Step 865: [1920/6413 (30%)] Loss: 0.457168
Step 875: [2240/6413 (35%)] Loss: 0.326938
Step 885: [2560/6413 (40%)] Loss: 0.408326
Step 895: [2880/6413 (45%)] Loss: 0.507172
Step 905: [3200/6413 (50%)] Loss: 0.588444
Step 915: [3520/6413 (55%)] Loss: 0.414341
Step 925: [3840/6413 (60%)] Loss: 0.502352
Step 935: [4160/6413 (65%)] Loss: 0.518117
Step 945: [4480/6413 (70%)] Loss: 0.352987
Step 955: [4800/6413 (75%)] Loss: 0.283002
Step 965: [5120/6413 (80%)] Loss: 0.562468
Step 975: [5440/6413 (85%)] Loss: 0.350719
Step 985: [5760/6413 (90%)] Loss: 0.598661
Step 995: [6080/6413 (95%)] Loss: 0.564264
Step 1005: [2600/6413 (100%)] Loss: 0.476096
ðŸŸ¢ Test Accuracy:  0.024644482622148665
ðŸ‘‰ Train Epoch 5:
Step 1006: [0/6413 (0%)] Loss: 0.615279
Step 1016: [320/6413 (5%)] Loss: 0.662243
Step 1026: [640/6413 (10%)] Loss: 0.470574
Step 1036: [960/6413 (15%)] Loss: 0.422844
Step 1046: [1280/6413 (20%)] Loss: 0.501589
Step 1056: [1600/6413 (25%)] Loss: 0.553367
Step 1066: [1920/6413 (30%)] Loss: 0.511237
Step 1076: [2240/6413 (35%)] Loss: 0.466602
Step 1086: [2560/6413 (40%)] Loss: 0.565345
Step 1096: [2880/6413 (45%)] Loss: 0.428859
Step 1106: [3200/6413 (50%)] Loss: 0.335829
Step 1116: [3520/6413 (55%)] Loss: 0.478955
Step 1126: [3840/6413 (60%)] Loss: 0.351724
Step 1136: [4160/6413 (65%)] Loss: 0.532083
Step 1146: [4480/6413 (70%)] Loss: 0.299963
Step 1156: [4800/6413 (75%)] Loss: 0.506489
Step 1166: [5120/6413 (80%)] Loss: 0.403715
Step 1176: [5440/6413 (85%)] Loss: 0.550961
Step 1186: [5760/6413 (90%)] Loss: 0.406121
Step 1196: [6080/6413 (95%)] Loss: 0.336315
Step 1206: [2600/6413 (100%)] Loss: 0.687900
ðŸŸ¢ Test Accuracy:  0.02448819889184716
ðŸ‘‰ Train Epoch 6:
Step 1207: [0/6413 (0%)] Loss: 0.449853
Step 1217: [320/6413 (5%)] Loss: 0.460226
Step 1227: [640/6413 (10%)] Loss: 0.378208
Step 1237: [960/6413 (15%)] Loss: 0.311565
Step 1247: [1280/6413 (20%)] Loss: 0.362131
Step 1257: [1600/6413 (25%)] Loss: 0.438379
Step 1267: [1920/6413 (30%)] Loss: 0.372230
Step 1277: [2240/6413 (35%)] Loss: 0.458886
Step 1287: [2560/6413 (40%)] Loss: 0.378277
Step 1297: [2880/6413 (45%)] Loss: 0.537801
Step 1307: [3200/6413 (50%)] Loss: 0.513159
Step 1317: [3520/6413 (55%)] Loss: 0.377542
Step 1327: [3840/6413 (60%)] Loss: 0.439138
Step 1337: [4160/6413 (65%)] Loss: 0.749815
Step 1347: [4480/6413 (70%)] Loss: 0.583158
Step 1357: [4800/6413 (75%)] Loss: 0.351560
Step 1367: [5120/6413 (80%)] Loss: 0.465992
Step 1377: [5440/6413 (85%)] Loss: 0.481880
Step 1387: [5760/6413 (90%)] Loss: 0.490741
Step 1397: [6080/6413 (95%)] Loss: 0.579278
Step 1407: [2600/6413 (100%)] Loss: 0.499177
ðŸŸ¢ Test Accuracy:  0.02484236705763834
ðŸ‘‰ Train Epoch 7:
Step 1408: [0/6413 (0%)] Loss: 0.423743
Step 1418: [320/6413 (5%)] Loss: 0.526868
Step 1428: [640/6413 (10%)] Loss: 0.559795
Step 1438: [960/6413 (15%)] Loss: 0.263112
Step 1448: [1280/6413 (20%)] Loss: 0.348564
Step 1458: [1600/6413 (25%)] Loss: 0.566075
Step 1468: [1920/6413 (30%)] Loss: 0.381011
Step 1478: [2240/6413 (35%)] Loss: 0.410372
Step 1488: [2560/6413 (40%)] Loss: 0.396186
Step 1498: [2880/6413 (45%)] Loss: 0.577733
Step 1508: [3200/6413 (50%)] Loss: 0.430088
Step 1518: [3520/6413 (55%)] Loss: 0.425598
Step 1528: [3840/6413 (60%)] Loss: 0.455894
Step 1538: [4160/6413 (65%)] Loss: 0.461627
Step 1548: [4480/6413 (70%)] Loss: 0.561792
Step 1558: [4800/6413 (75%)] Loss: 0.344328
Step 1568: [5120/6413 (80%)] Loss: 0.531204
Step 1578: [5440/6413 (85%)] Loss: 0.423423
Step 1588: [5760/6413 (90%)] Loss: 0.263932
Step 1598: [6080/6413 (95%)] Loss: 0.428575
Step 1608: [2600/6413 (100%)] Loss: 0.465635
ðŸŸ¢ Test Accuracy:  0.02467821292365259
ðŸ‘‰ Train Epoch 8:
Step 1609: [0/6413 (0%)] Loss: 0.500540
Step 1619: [320/6413 (5%)] Loss: 0.431586
Step 1629: [640/6413 (10%)] Loss: 0.484081
Step 1639: [960/6413 (15%)] Loss: 0.273646
Step 1649: [1280/6413 (20%)] Loss: 0.448507
Step 1659: [1600/6413 (25%)] Loss: 0.237001
Step 1669: [1920/6413 (30%)] Loss: 0.272892
Step 1679: [2240/6413 (35%)] Loss: 0.471091
Step 1689: [2560/6413 (40%)] Loss: 0.327275
Step 1699: [2880/6413 (45%)] Loss: 0.274417
Step 1709: [3200/6413 (50%)] Loss: 0.389423
Step 1719: [3520/6413 (55%)] Loss: 0.331657
Step 1729: [3840/6413 (60%)] Loss: 0.475019
Step 1739: [4160/6413 (65%)] Loss: 0.411533
Step 1749: [4480/6413 (70%)] Loss: 0.319233
Step 1759: [4800/6413 (75%)] Loss: 0.490705
Step 1769: [5120/6413 (80%)] Loss: 0.459494
Step 1779: [5440/6413 (85%)] Loss: 0.525613
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x0000027EE4C07CD0>> (for post_run_cell), with arguments args (<ExecutionResult object at 27ee4b2e380, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 27ee4b2dd80, raw_cell="def __jupyter_exec_background__():
    from IPytho.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
