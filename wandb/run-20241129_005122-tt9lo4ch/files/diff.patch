warning: in the working copy of 'cd0387_common_model_arch_types_fine_tuning/finetune_bert_solution.ipynb', LF will be replaced by CRLF the next time Git touches it
diff --git a/cd0387_common_model_arch_types_fine_tuning/finetune_bert_solution.ipynb b/cd0387_common_model_arch_types_fine_tuning/finetune_bert_solution.ipynb
index a0a354d..96dfd6d 100644
--- a/cd0387_common_model_arch_types_fine_tuning/finetune_bert_solution.ipynb
+++ b/cd0387_common_model_arch_types_fine_tuning/finetune_bert_solution.ipynb
@@ -8,12 +8,14 @@
     "* changed by nov05 on 2024-11-28  \n",
     "* Udacity AWS MLE Nanodegree (ND189)  \n",
     "  Course 4, 3.15 Excercise: Fine-Tuning BERT    \n",
-    "* `conda activate drlnd_py310` with cuda enabled  "
+    "* local env `conda activate cuda_py310` with cuda enabled  \n",
+    "* [CoLA dataset on KaggleHub](https://www.kaggle.com/datasets/krazy47/cola-the-corpus-of-linguistic-acceptability)  \n",
+    "  The Corpus of Linguistic Acceptability   "
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 4,
    "id": "bb5575d2",
    "metadata": {},
    "outputs": [
@@ -21,16 +23,16 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "D:\\github\\udacity-CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter\n"
+      "d:\\github\\udacity-CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "'D:\\\\github\\\\udacity-CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter'"
+       "'d:\\\\github\\\\udacity-CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter'"
       ]
      },
-     "execution_count": 3,
+     "execution_count": 4,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -47,19 +49,20 @@
    "id": "ece7b442",
    "metadata": {},
    "source": [
-    "## Solution: Finetune BERT model"
+    "## Solution: Fine-tune BERT model"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 1,
    "id": "20247d44",
    "metadata": {},
    "outputs": [],
    "source": [
-    "import json\n",
     "import os\n",
-    "import sys\n",
+    "# import sys\n",
+    "# import json\n",
+    "from tqdm import tqdm\n",
     "import wandb\n",
     "\n",
     "import numpy as np\n",
@@ -69,43 +72,167 @@
     "import torch.utils.data\n",
     "import torch.utils.data.distributed\n",
     "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
-    "from transformers import BertForSequenceClassification, BertTokenizer\n",
-    "from sklearn.model_selection import train_test_split"
+    "from transformers import BertForSequenceClassification, BertTokenizer # type: ignore\n",
+    "from transformers import get_linear_schedule_with_warmup # type: ignore\n",
+    "from sklearn.model_selection import train_test_split # type: ignore\n",
+    "\n",
+    "## log training process with W&B if uncommented\n",
+    "# os.environ['WANDB_MODE'] = 'disabled'"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 2,
    "id": "23828261",
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "ðŸ‘‰ Running on device type: cuda:0\n"
+     ]
+    }
+   ],
    "source": [
     "class Config:\n",
     "    def __init__(self):\n",
-    "        self.wandb = True\n",
+    "        # self.wandb = True\n",
     "        self.device = torch.device('cpu')\n",
     "        self.max_len = 64 ## this is the max length of the sentence\n",
     "        self.epochs = 30\n",
-    "        self.batch_size = 64\n",
+    "        self.batch_size = 32\n",
     "        self.opt_lr = 2e-5\n",
-    "        self.opt_weight_decay = 1e-4\n",
-    "        self.unfreeze_top_layers = True\n",
+    "        self.opt_weight_decay = 1e-2\n",
     "\n",
     "config = Config()\n",
     "config.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
     "print(f\"ðŸ‘‰ Running on device type: {config.device}\")"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "id": "9a15cc9a",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "(8551,) (8551,)\n"
+     ]
+    },
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>label</th>\n",
+       "      <th>sentence</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <th>6550</th>\n",
+       "      <td>0</td>\n",
+       "      <td>Talkative and a bully entered.</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>8541</th>\n",
+       "      <td>1</td>\n",
+       "      <td>Where has he put the cake?</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>7311</th>\n",
+       "      <td>1</td>\n",
+       "      <td>I know that she runs.</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "      label                        sentence\n",
+       "6550      0  Talkative and a bully entered.\n",
+       "8541      1      Where has he put the cake?\n",
+       "7311      1           I know that she runs."
+      ]
+     },
+     "execution_count": 5,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "df = pd.read_csv(\n",
+    "   r\"cd0387_common_model_arch_types_fine_tuning\\cola_public\\raw\\in_domain_train.tsv\",\n",
+    "   sep=\"\\t\",\n",
+    "   header=None,\n",
+    "   usecols=[1, 3],\n",
+    "   names=[\"label\", \"sentence\"],\n",
+    ")\n",
+    "sentences = df.sentence.values\n",
+    "labels = df.label.values\n",
+    "print(sentences.shape, labels.shape)\n",
+    "df.sample(3)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "id": "1c7d8d8c",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "train_df, test_df = train_test_split(df)\n",
+    "train_df.to_csv(r\"data\\cola\\train.csv\", index=False)\n",
+    "test_df.to_csv(r\"data\\cola\\test.csv\", index=False)"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "7e22c3cf",
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Loading BERT tokenizer...\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnov05\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
+     ]
+    }
+   ],
    "source": [
     "print(\"Loading BERT tokenizer...\")\n",
     "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
     "\n",
+    "\n",
     "def flat_accuracy(preds, labels):\n",
     "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
     "    labels_flat = labels.flatten()\n",
@@ -113,7 +240,7 @@
     "\n",
     "\n",
     "def get_train_data_loader(batch_size):\n",
-    "    dataset = pd.read_csv(os.path.join(\"data\", \"train.csv\"))\n",
+    "    dataset = pd.read_csv(os.path.join(\"data\", \"cola\", \"train.csv\"))\n",
     "    sentences = dataset.sentence.values\n",
     "    labels = dataset.label.values\n",
     "\n",
@@ -150,7 +277,7 @@
     "\n",
     "\n",
     "def get_test_data_loader(test_batch_size):\n",
-    "    dataset = pd.read_csv(os.path.join(\"data\", \"test.csv\"))\n",
+    "    dataset = pd.read_csv(os.path.join(\"data\", \"cola\", \"test.csv\"))\n",
     "    sentences = dataset.sentence.values\n",
     "    labels = dataset.label.values\n",
     "\n",
@@ -196,12 +323,24 @@
     "        output_attentions=False,  # Whether the model returns attentions weights\n",
     "        output_hidden_states=False,  # Whether the model returns all hidden-states\n",
     "    )\n",
+    "    for param in model.bert.parameters():\n",
+    "        param.requires_grad = False  # Freeze all BERT layers\n",
+    "    for param in model.bert.encoder.layer[-1].parameters():\n",
+    "        param.requires_grad = True  # Unfreeze the last layer\n",
+    "\n",
     "\n",
     "    model = model.to(config.device)\n",
-    "    optimizer = torch.optim.AdamW(model.parameters(), config.opt_lr)\n",
+    "    optimizer = torch.optim.AdamW(\n",
+    "        model.parameters(), lr=config.opt_lr, weight_decay=config.opt_weight_decay)\n",
+    "    scheduler = get_linear_schedule_with_warmup(\n",
+    "        optimizer,\n",
+    "        num_warmup_steps=0,  # You can start with 0 warmup steps, or adjust this\n",
+    "        num_training_steps=len(train_loader)*config.epochs\n",
+    "    )\n",
     "    total_steps = 0\n",
     "\n",
-    "    for epoch in range(1, config.epochs+1):\n",
+    "    for epoch in tqdm(range(config.epochs)):\n",
+    "        print(f\"ðŸ‘‰ Train Epoch {epoch}:\")\n",
     "        total_loss = 0\n",
     "        model.train()\n",
     "        for step, batch in enumerate(train_loader):\n",
@@ -211,81 +350,60 @@
     "            b_labels = batch[2].to(config.device)\n",
     "            model.zero_grad()\n",
     "\n",
-    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
+    "            outputs = model(b_input_ids, token_type_ids=None, \n",
+    "                attention_mask=b_input_mask, labels=b_labels)\n",
     "            loss = outputs[0]\n",
     "\n",
     "            total_loss += loss.item()\n",
+    "            wandb.log({\"train_loss\": loss.item()}, step=total_steps)\n",
     "            loss.backward()\n",
-    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
+    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
     "            # modified based on their gradients, the learning rate, etc.\n",
     "            optimizer.step()\n",
-    "            if step % 10  == 0:\n",
+    "            if step%10==0:\n",
     "                print(\n",
-    "                    f\"Train Epoch {epoch}:  \" \n",
+    "                    f\"Step {total_steps}: \" \n",
     "                    f\"[{step*len(batch[0])}/{len(train_loader.sampler)} \" \n",
     "                    f\"({(100.0*step/len(train_loader)):.0f}%)] \" \n",
     "                    f\"Loss: {loss.item():.6f}\"\n",
     "                )\n",
+    "        scheduler.step()\n",
+    "        eval_accuracy = test(model, test_loader)\n",
+    "        wandb.log({f\"eval_accuracy_epoch\": eval_accuracy}, step=total_steps)\n",
+    "    return model\n",
     "\n",
-    "    test(model, test_loader)\n",
     "\n",
     "def test(model, test_loader):\n",
     "    model.eval()\n",
     "    _, eval_accuracy = 0, 0\n",
-    "\n",
     "    with torch.no_grad():\n",
     "        for batch in test_loader:\n",
     "            b_input_ids = batch[0].to(config.device)\n",
     "            b_input_mask = batch[1].to(config.device)\n",
     "            b_labels = batch[2].to(config.device)\n",
-    "\n",
     "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
     "            logits = outputs[0]\n",
     "            logits = logits.detach().cpu().numpy()\n",
     "            label_ids = b_labels.to(\"cpu\").numpy()\n",
     "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
     "            eval_accuracy += tmp_eval_accuracy\n",
+    "    eval_accuracy /= len(test_loader.dataset)\n",
+    "    print(\"ðŸŸ¢ Test Accuracy: \", eval_accuracy)\n",
+    "    return eval_accuracy\n",
     "\n",
-    "    print(\"Test Accuracy: \", eval_accuracy/len(test_loader.dataset))"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "id": "202f5cef",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "df = pd.read_csv(\n",
-    "   \"./cola_public/raw/in_domain_train.tsv\",\n",
-    "   sep=\"\\t\",\n",
-    "   header=None,\n",
-    "   usecols=[1, 3],\n",
-    "   names=[\"label\", \"sentence\"],\n",
+    "\n",
+    "wandb.init(\n",
+    "    # set the wandb project where this run will be logged\n",
+    "    project=\"udacity-awsmle-bert-cola\",\n",
+    "    config=config\n",
     ")\n",
-    "sentences = df.sentence.values\n",
-    "labels = df.label.values"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "id": "6a652150",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "train_df, test_df = train_test_split(df)\n",
-    "train_df.to_csv(\"./data/train.csv\", index=False)\n",
-    "test_df.to_csv(\"./data/test.csv\", index=False)\n",
-    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
-    "print(device)\n",
-    "train(device)"
+    "train()"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
-   "id": "dae07804",
+   "id": "e02af93e",
    "metadata": {},
    "outputs": [],
    "source": []
@@ -293,7 +411,7 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "drlnd_py310",
+   "display_name": "cuda_py310",
    "language": "python",
    "name": "python3"
   },
