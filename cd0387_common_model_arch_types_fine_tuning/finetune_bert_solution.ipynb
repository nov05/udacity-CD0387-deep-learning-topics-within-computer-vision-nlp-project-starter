{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079808d1",
   "metadata": {},
   "source": [
    "* changed by nov05 on 2024-11-28  \n",
    "* Udacity AWS MLE Nanodegree (ND189)  \n",
    "  Course 4, 3.15 Excercise: Fine-Tuning BERT    \n",
    "* `conda activate drlnd_py310` with cuda enabled  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5575d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\github\\udacity-CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\github\\\\udacity-CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use the repo root folder as working directory\n",
    "## training data is in data\\\n",
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7b442",
   "metadata": {},
   "source": [
    "## Solution: Finetune BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20247d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23828261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.wandb = True\n",
    "        self.device = torch.device('cpu')\n",
    "        self.max_len = 64 ## this is the max length of the sentence\n",
    "        self.epochs = 30\n",
    "        self.batch_size = 64\n",
    "        self.opt_lr = 2e-5\n",
    "        self.opt_weight_decay = 1e-4\n",
    "        self.unfreeze_top_layers = True\n",
    "\n",
    "config = Config()\n",
    "config.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ‘‰ Running on device type: {config.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def get_train_data_loader(batch_size):\n",
    "    dataset = pd.read_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "    sentences = dataset.sentence.values\n",
    "    labels = dataset.label.values\n",
    "\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for id in input_ids:\n",
    "        while len(id) < config.max_len:\n",
    "            id.append(0)\n",
    "        input_ids_padded.append(id)\n",
    "    input_ids = input_ids_padded\n",
    "\n",
    "    # mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    # convert to PyTorch data types.\n",
    "    train_inputs = torch.tensor(input_ids)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def get_test_data_loader(test_batch_size):\n",
    "    dataset = pd.read_csv(os.path.join(\"data\", \"test.csv\"))\n",
    "    sentences = dataset.sentence.values\n",
    "    labels = dataset.label.values\n",
    "\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for id in input_ids:\n",
    "        while len(id) < config.max_len:\n",
    "            id.append(0)\n",
    "        input_ids_padded.append(id)\n",
    "    input_ids = input_ids_padded\n",
    "\n",
    "    # mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    # convert to PyTorch data types.\n",
    "    train_inputs = torch.tensor(input_ids)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=test_batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def train():\n",
    "    train_loader = get_train_data_loader(config.batch_size)\n",
    "    test_loader = get_test_data_loader(config.batch_size)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels=2,  # The number of output labels--2 for binary classification.\n",
    "        output_attentions=False,  # Whether the model returns attentions weights\n",
    "        output_hidden_states=False,  # Whether the model returns all hidden-states\n",
    "    )\n",
    "\n",
    "    model = model.to(config.device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), config.opt_lr)\n",
    "    total_steps = 0\n",
    "\n",
    "    for epoch in range(1, config.epochs+1):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            total_steps += 1\n",
    "            b_input_ids = batch[0].to(config.device)\n",
    "            b_input_mask = batch[1].to(config.device)\n",
    "            b_labels = batch[2].to(config.device)\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "            if step % 10  == 0:\n",
    "                print(\n",
    "                    f\"Train Epoch {epoch}:  \" \n",
    "                    f\"[{step*len(batch[0])}/{len(train_loader.sampler)} \" \n",
    "                    f\"({(100.0*step/len(train_loader)):.0f}%)] \" \n",
    "                    f\"Loss: {loss.item():.6f}\"\n",
    "                )\n",
    "\n",
    "    test(model, test_loader)\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    _, eval_accuracy = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            b_input_ids = batch[0].to(config.device)\n",
    "            b_input_mask = batch[1].to(config.device)\n",
    "            b_labels = batch[2].to(config.device)\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to(\"cpu\").numpy()\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    print(\"Test Accuracy: \", eval_accuracy/len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "   \"./cola_public/raw/in_domain_train.tsv\",\n",
    "   sep=\"\\t\",\n",
    "   header=None,\n",
    "   usecols=[1, 3],\n",
    "   names=[\"label\", \"sentence\"],\n",
    ")\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a652150",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df)\n",
    "train_df.to_csv(\"./data/train.csv\", index=False)\n",
    "test_df.to_csv(\"./data/test.csv\", index=False)\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "train(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae07804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
