{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079808d1",
   "metadata": {},
   "source": [
    "* changed by nov05 on 2024-11-28  \n",
    "\n",
    "* Udacity AWS MLE Nanodegree (ND189)  \n",
    "  Course 4, 3.15 Excercise: Fine-Tuning BERT    \n",
    "  \n",
    "* ~~local env `conda activate cuda_py310` with cuda enabled~~  \n",
    "  moved to `google colab` for traning on better GPU   \n",
    "  \n",
    "* [CoLA dataset on KaggleHub](https://www.kaggle.com/datasets/krazy47/cola-the-corpus-of-linguistic-acceptability)  \n",
    "  The Corpus of Linguistic Acceptability    \n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5944ece",
   "metadata": {},
   "source": [
    "* ðŸ‘‰ Check [the `baseline` notebook in github](https://github.com/nov05/Google-Colaboratory/blob/master/20241129_finetune_bert_solution_2_baseline.ipynb), or [run it in colab](https://drive.google.com/file/d/11V8yjHs5rna2Kg8DHMhnm3XEBbR9m2mY)  \n",
    "  Trained all 15 layers of the BERT-base-uncased model   \n",
    "* Check the [W&B training logs](https://wandb.ai/nov05/udacity-awsmle-bert-cola/runs/3blkneau)   \n",
    "  Run summary:  \n",
    "  ```\n",
    "  eval_accuracy_epoch (%)\t81.44089\n",
    "  train_loss\t0.00013\n",
    "  train_loss_epoch\t0.02814 \n",
    "  epochs: 15\n",
    "  time: 16m 21s  \n",
    "  ```\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/nov05/pictures/refs/heads/master/Udacity/20241119_aws-mle-nanodegree/2024-11-29%2007_33_08-solar-thunder-20-baseline%20_%20udacity-awsmle-bert-cola%20%E2%80%93%20Weights%20%26%20Biases.jpg\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dd4e63",
   "metadata": {},
   "source": [
    "* ðŸ‘‰ Check [the `partial freeze` notebook in github](https://github.com/nov05/Google-Colaboratory/blob/master/20241129_finetune_bert_solution_3_partial_freeze.ipynb), or [run it in in colab](https://drive.google.com/file/d/1xx7XdqVo7lY2El44b20oiH41u9GQbpAC)    \n",
    "  Freeze 6 top layers: [classifier, pooler, encoder.layer.11-8]. Train the other layers of the BERT-base-uncased model  \n",
    "\n",
    "* Check the [W&B training logs](https://wandb.ai/nov05/udacity-awsmle-bert-cola/runs/72jzrfqq)  \n",
    "  Run summary:  \n",
    "  ```\n",
    "  eval_accuracy_epoch (%)\t81.11567\n",
    "  train_loss\t0.26174\n",
    "  train_loss_epoch\t0.17329\n",
    "  epochs: 15\n",
    "  time: 10m 38s\n",
    "  ```  \n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/nov05/pictures/refs/heads/master/Udacity/20241119_aws-mle-nanodegree/2024-11-29%2017_18_34-ethereal-leaf-21%20_%20udacity-awsmle-bert-cola%20%E2%80%93%20Weights%20%26%20Biases.jpg\" width=600>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115cb3b",
   "metadata": {},
   "source": [
    "* Check [the W&B report](https://api.wandb.ai/links/nov05/nsw4fhpq)  \n",
    "  <img src=\"https://raw.githubusercontent.com/nov05/pictures/refs/heads/master/Udacity/20241119_aws-mle-nanodegree/2024-11-29%2017_52_11-Settings.jpg\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440842c0",
   "metadata": {},
   "source": [
    "**ðŸŸ¢ Conclusion**ï¼š Partially freezing the pre-trained model can significantly speed up training without sacrificing result quality.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2356a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
