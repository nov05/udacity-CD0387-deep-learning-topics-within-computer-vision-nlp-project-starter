{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece7b442",
   "metadata": {},
   "source": [
    "## Solution: Finetune BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20247d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64  # this is the max length of the sentence\n",
    "batch_size=64\n",
    "epochs=1\n",
    "\n",
    "print(\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def get_train_data_loader(batch_size):\n",
    "    dataset = pd.read_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "    sentences = dataset.sentence.values\n",
    "    labels = dataset.label.values\n",
    "\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for i in input_ids:\n",
    "        while len(i) < MAX_LEN:\n",
    "            i.append(0)\n",
    "        input_ids_padded.append(i)\n",
    "    input_ids = input_ids_padded\n",
    "\n",
    "    # mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    # convert to PyTorch data types.\n",
    "    train_inputs = torch.tensor(input_ids)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def get_test_data_loader(test_batch_size):\n",
    "    dataset = pd.read_csv(os.path.join(\"data\", \"test.csv\"))\n",
    "    sentences = dataset.sentence.values\n",
    "    labels = dataset.label.values\n",
    "\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for i in input_ids:\n",
    "        while len(i) < MAX_LEN:\n",
    "            i.append(0)\n",
    "        input_ids_padded.append(i)\n",
    "    input_ids = input_ids_padded\n",
    "\n",
    "    # mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    # convert to PyTorch data types.\n",
    "    train_inputs = torch.tensor(input_ids)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=test_batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def train(device):\n",
    "    train_loader = get_train_data_loader(batch_size)\n",
    "    test_loader = get_test_data_loader(batch_size)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels=2,  # The number of output labels--2 for binary classification.\n",
    "        output_attentions=False,  # Whether the model returns attentions weights.\n",
    "        output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "            if step % 10  == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        step * len(batch[0]),\n",
    "                        len(train_loader.sampler),\n",
    "                        100.0 * step / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    test(model, test_loader, device)\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    _, eval_accuracy = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to(\"cpu\").numpy()\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    print(\"Test set: Accuracy: \", eval_accuracy/len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "   \"./cola_public/raw/in_domain_train.tsv\",\n",
    "   sep=\"\\t\",\n",
    "   header=None,\n",
    "   usecols=[1, 3],\n",
    "   names=[\"label\", \"sentence\"],\n",
    ")\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a652150",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df)\n",
    "train_df.to_csv(\"./data/train.csv\", index=False)\n",
    "test_df.to_csv(\"./data/test.csv\", index=False)\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "train(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae07804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
