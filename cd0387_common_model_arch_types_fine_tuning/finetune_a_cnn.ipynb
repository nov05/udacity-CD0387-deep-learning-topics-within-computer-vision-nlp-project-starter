{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a Convolutional Neural Network\n",
    "\n",
    "In this exercise, you will have to finetune a pretrained CNN model on the CIFAR10 dataset. The data loading and model testing logic are already included in your code. You will have to create the model and the training loop.\n",
    "\n",
    "**In this workspace you have GPU to help train the model but it is best practice to DISABLE it while writing code and only ENABLE it when you are training.** \n",
    "\n",
    "Here are the steps you need to do to complete this exercise:\n",
    "\n",
    "1. Finish the `create_model()` function. You should use a pretrained model. You are free to choose any pre-trained model that you want to use. \n",
    "2. Finish the `train()` function. This function should validate the accuracy of the model during the training stage. You should stop the training when this validation accuracy stops increasing.\n",
    "3. Save all your work and then **ENABLE** the GPU\n",
    "4. Run the file to make sure that the model is training properly.\n",
    "5. If it works, remember to **DISABLE** the GPU before moving to the next page. \n",
    "\n",
    "In case you get stuck, you can look at the solution by clicking the jupyter symbol at the top left and navigating to `finetune_a_cnn_solution.py`.\n",
    "\n",
    "## Try It Out!\n",
    "- See how your accuracy changes when using other pre-trained models.\n",
    "- Play around with the number of layers and neurons in your model. How does the accuracy change? How long does it take to train the model?\n",
    "- Can you create the same network in TensorFlow as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss=0\n",
    "    running_corrects=0\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        outputs=model(inputs)\n",
    "        loss=criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "    total_loss = running_loss / len(test_loader)\n",
    "    total_acc = running_corrects / len(test_loader)\n",
    "    \n",
    "def train(model, train_loader, validation_loader, criterion, optimizer):\n",
    "    epochs=50\n",
    "    best_loss=1e6\n",
    "    image_dataset={'train':train_loader, 'valid':validation_loader}\n",
    "    loss_counter=0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for phase in ['train', 'valid']:\n",
    "            print(f\"Epoch {epoch}, Phase {phase}\")\n",
    "            if phase=='train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "        #TODO: Finish the rest of the training code\n",
    "        # The code should stop training when the validation accuracy\n",
    "        # stops increasing\n",
    "    return model\n",
    "\n",
    "def create_model():\n",
    "    # TODO: Write code to create the model\n",
    "\n",
    "    return model\n",
    "\n",
    "batch_size=32\n",
    "training_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "testing_transform = transforms.Compose([transforms.ToTensor(),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "        download=True, transform=training_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "        download=True, transform=testing_transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "\n",
    "model=create_model()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "train(model, trainloader, testloader, criterion, optimizer)\n",
    "\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to Disable GPU when you are done training. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
