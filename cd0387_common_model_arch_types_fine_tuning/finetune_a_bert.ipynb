{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a BERT Model\n",
    "\n",
    "In this exercise, you will have to create a BERT model and then train it on the CoLA dataset which has a list of sentences that are either grammatically correct or incorrect. The data loading and model training, testing logic are already included in your code. You will need to fetch the data, and use a pretrained BERT model to solve a classification task.\n",
    "\n",
    "**In this workspace you have GPU to help train the model but it is best practice to DISABLE it while writing code and only ENABLE it when you are training.**\n",
    "\n",
    "Here are the steps you need to do to complete this exercise:\n",
    "\n",
    "1. Data has already been downloaded from [here](https://nyu-mll.github.io/CoLA/) into the `cola_public` directory.\n",
    "2. Create a tokenizer for the BERT Model\n",
    "3. Create the BERT Model and the optimizer for the model\n",
    "4. Save all your work and then **ENABLE** the GPU\n",
    "5. Run the Package Installations.\n",
    "5. Run the file to make sure that the model is training properly.\n",
    "6. If it works, remember to **DISABLE** the GPU before moving to the next page. \n",
    "\n",
    "In case you get stuck, you can look at the solution by clicking the jupyter symbol at the top left and navigating to `finetune_a_bert_solution.py`.\n",
    "\n",
    "## Try It Out!\n",
    "- Can you train a different BERT architecture and compare the results? \n",
    "- Can you finetune the same model on a different dataset or task and compare the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installations\n",
    "**NOTE**: Everytime you start the GPU, run this before your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pip --upgrade\n",
    "!pip install datasets==1.8.0\n",
    "!pip install transformers==4.6.1\n",
    "!pip install ipywidgets\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Code\n",
    "\n",
    "**Remember** to DISABLE the GPU when you are not training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "from transformers import AdamW, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "MAX_LEN = 64  # this is the max length of the sentence\n",
    "batch_size=64\n",
    "epochs=1\n",
    "\n",
    "tokenizer = #TODO: Create the BERT tokenizer\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def get_train_data_loader(batch_size):\n",
    "    dataset = pd.read_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "    sentences = dataset.sentence.values\n",
    "    labels = dataset.label.values\n",
    "\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for i in input_ids:\n",
    "        while len(i) < MAX_LEN:\n",
    "            i.append(0)\n",
    "        input_ids_padded.append(i)\n",
    "    input_ids = input_ids_padded\n",
    "\n",
    "    # mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    # convert to PyTorch data types.\n",
    "    train_inputs = torch.tensor(input_ids)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def get_test_data_loader(test_batch_size):\n",
    "    dataset = pd.read_csv(os.path.join(\"data\", \"test.csv\"))\n",
    "    sentences = dataset.sentence.values\n",
    "    labels = dataset.label.values\n",
    "\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for i in input_ids:\n",
    "        while len(i) < MAX_LEN:\n",
    "            i.append(0)\n",
    "        input_ids_padded.append(i)\n",
    "    input_ids = input_ids_padded\n",
    "\n",
    "    # mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    # convert to PyTorch data types.\n",
    "    train_inputs = torch.tensor(input_ids)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=test_batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    _, eval_accuracy = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to(\"cpu\").numpy()\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    print(\"Test set: Accuracy: \", eval_accuracy/len(test_loader.dataset))\n",
    "\n",
    "def train(device):\n",
    "    train_loader = get_train_data_loader(batch_size)\n",
    "    test_loader = get_test_data_loader(batch_size)\n",
    "\n",
    "    model = #TODO: Create the BERT Model\n",
    "    model=model.to(device)\n",
    "    optimizer = #TODO: Create the optimizer\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "            if step % 10  == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        step * len(batch[0]),\n",
    "                        len(train_loader.sampler),\n",
    "                        100.0 * step / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    test(model, test_loader, device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\n",
    "        \"./cola_public/raw/in_domain_train.tsv\",\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        usecols=[1, 3],\n",
    "        names=[\"label\", \"sentence\"],\n",
    "    )\n",
    "    sentences = df.sentence.values\n",
    "    labels = df.label.values\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    train_df.to_csv(\"./data/train.csv\", index=False)\n",
    "    test_df.to_csv(\"./data/test.csv\", index=False)\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
