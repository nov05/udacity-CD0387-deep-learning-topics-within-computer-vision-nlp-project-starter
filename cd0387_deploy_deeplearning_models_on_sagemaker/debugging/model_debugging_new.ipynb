{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2b9457",
   "metadata": {},
   "source": [
    "* created by nov05 on 2024-11-28  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827128cc-9369-4d8d-b37f-6ab4c439fe89",
   "metadata": {},
   "source": [
    "# SageMaker Model Debugging\n",
    "\n",
    "Here we will see how we can use Sagemaker Debugging to see our model training performance as well as generate a simple report called the Profiler Report that gives us an overview of our training job.\n",
    "\n",
    "First we will need to install `smdebug`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff9e869-fff6-4ac3-825a-ba1da258c5b7",
   "metadata": {},
   "source": [
    "## `pytorch_mnist.py`\n",
    "<details>\n",
    "  <summary> Click here to see the full script code </summary>\n",
    "   \n",
    "``` python\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ====================================#\n",
    "# 1. Import SMDebug framework class. #\n",
    "# ====================================#\n",
    "import smdebug.pytorch as smd\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch, hook):\n",
    "    model.train()\n",
    "    # =================================================#\n",
    "    # 2. Set the SMDebug hook for the training phase. #\n",
    "    # =================================================#\n",
    "    hook.set_mode(smd.modes.TRAIN)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "def test(model, test_loader, hook):\n",
    "    model.eval()\n",
    "    # ===================================================#\n",
    "    # 3. Set the SMDebug hook for the validation phase. #\n",
    "    # ===================================================#\n",
    "    hook.set_mode(smd.modes.EVAL)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for training (default: 64)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for testing (default: 1000)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=14,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 14)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", type=float, default=1.0, metavar=\"LR\", help=\"learning rate (default: 1.0)\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    train_kwargs = {\"batch_size\": args.batch_size}\n",
    "    test_kwargs = {\"batch_size\": args.test_batch_size}\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "    dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net()\n",
    "\n",
    "    # ======================================================#\n",
    "    # 4. Register the SMDebug hook to save output tensors. #\n",
    "    # ======================================================#\n",
    "    hook = smd.Hook.create_from_json_file()\n",
    "    hook.register_hook(model)\n",
    "\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        # ===========================================================#\n",
    "        # 5. Pass the SMDebug hook to the train and test functions. #\n",
    "        # ===========================================================#\n",
    "        train(model, train_loader, optimizer, epoch, hook)\n",
    "        test(model, test_loader, hook)\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae696ff-881b-4bad-9662-466eaeae2620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting smdebug\n",
      "  Downloading smdebug-1.0.34-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting protobuf<=3.20.3,>=3.20.0 (from smdebug)\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-win_amd64.whl.metadata (698 bytes)\n",
      "Requirement already satisfied: numpy>=1.16.0 in d:\\users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages (from smdebug) (1.26.4)\n",
      "Requirement already satisfied: packaging in d:\\users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages (from smdebug) (24.2)\n",
      "Requirement already satisfied: boto3>=1.10.32 in d:\\users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages (from smdebug) (1.35.66)\n",
      "Collecting pyinstrument==3.4.2 (from smdebug)\n",
      "  Downloading pyinstrument-3.4.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pyinstrument-cext>=0.2.2 (from pyinstrument==3.4.2->smdebug)\n",
      "  Downloading pyinstrument_cext-0.2.4.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.66 in d:\\users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages (from boto3>=1.10.32->smdebug) (1.35.66)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages (from boto3>=1.10.32->smdebug) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in d:\\users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages (from boto3>=1.10.32->smdebug) (0.10.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages (from botocore<1.36.0,>=1.35.66->boto3>=1.10.32->smdebug) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in d:\\users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages (from botocore<1.36.0,>=1.35.66->boto3>=1.10.32->smdebug) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.66->boto3>=1.10.32->smdebug) (1.16.0)\n",
      "Downloading smdebug-1.0.34-py2.py3-none-any.whl (280 kB)\n",
      "Downloading pyinstrument-3.4.2-py2.py3-none-any.whl (83 kB)\n",
      "Using cached protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "Building wheels for collected packages: pyinstrument-cext\n",
      "  Building wheel for pyinstrument-cext (setup.py): started\n",
      "  Building wheel for pyinstrument-cext (setup.py): finished with status 'done'\n",
      "  Created wheel for pyinstrument-cext: filename=pyinstrument_cext-0.2.4-cp310-cp310-win_amd64.whl size=8297 sha256=b51e9d26036648675999c0493c84b25a2972150400f2db8d68d126bc86d10b8a\n",
      "  Stored in directory: c:\\users\\guido\\appdata\\local\\pip\\cache\\wheels\\0f\\8b\\7a\\5f7fd1dd6d3cbb3d350d4c832c5e2f962687749f6d67d573a6\n",
      "Successfully built pyinstrument-cext\n",
      "Installing collected packages: pyinstrument-cext, pyinstrument, protobuf, smdebug\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "Successfully installed protobuf-3.20.3 pyinstrument-3.4.2 pyinstrument-cext-0.2.4 smdebug-1.0.34\n"
     ]
    }
   ],
   "source": [
    "!pip install smdebug\n",
    "## Successfully installed protobuf-3.20.3 pyinstrument-3.4.2 pyinstrument-cext-0.2.4 smdebug-1.0.34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ae13b-e972-4475-a6ab-153a24d7308d",
   "metadata": {},
   "source": [
    "## Debugger Rule and Configs\n",
    "\n",
    "Next we need to import the packages we will need and specify the debugger rules and configs. We will check for overfitting, overtraining, poor weight initialization and vanishing gradients. We will also set a save interval of 100 and 10 for training and testing respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e2cc47-3f0c-4ae1-baef-2b9b78a169dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\pydantic\\_internal\\_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/28/24 13:15:56] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file://D:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\botocore\\credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://D:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\botocore\\credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/28/24 13:15:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=653839;file://D:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\botocore\\credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=125942;file://D:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\botocore\\credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\guido\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.debugger import (\n",
    "    Rule,\n",
    "    DebuggerHookConfig,\n",
    "    rule_configs,\n",
    ")\n",
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs\n",
    "rules = [\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    Rule.sagemaker(rule_configs.poor_weight_initialization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "]\n",
    "hook_config = DebuggerHookConfig(\n",
    "    hook_parameters={\"train.save_interval\": \"100\", \"eval.save_interval\": \"10\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8f96d-421f-4d05-916c-a84dba2863de",
   "metadata": {},
   "source": [
    "Next we will specify the hyperparameters and create our estimator. In our estimator, we will additionally need to specify the debugger rules and configs that we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5fb46b-f81a-4ed1-a0a2-81d44b0dc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"epochs\": \"2\", \n",
    "                   \"batch-size\": \"32\", \n",
    "                   \"test-batch-size\": \"100\", \n",
    "                   \"lr\": \"0.001\"\n",
    "}\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"pytorch_mnist.py\",\n",
    "    base_job_name=\"smdebugger-mnist-pytorch\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    framework_version=\"1.8\",\n",
    "    py_version=\"py36\",\n",
    "    ## Debugger parameters\n",
    "    rules=rules,\n",
    "    debugger_hook_config=hook_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eafa46-8bfc-4940-b087-3ff61ce0919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awsmle_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
